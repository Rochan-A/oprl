model: file # one_state, five_state, gridworld, file
# if using gridworld, specify the gridworld env name
# eg. MiniGrid-SimpleCrossingS11N5-v0 MiniGrid-Empty-8x8-v0
# MiniGrid-LavaCrossingS11N5-v0 MiniGrid-LavaGapS7-v0 MiniGrid-FourRooms-v0
env: MiniGrid-Empty-8x8-v0
map_path: ./maps/rooms.txt
gamma: 0.9

# Environment Modifiers
env_mods:
  # (num of timesteps after reward is actually returned, except for terminal state)
  delay: None # inf, int, None

  # for all ball states, return stochastic reward
  stoch_reward:
    mean: 1
    std: 0.4

  # stochastic transition on box states
  stoch_transition:
    prob: [0.1, 0.2, 0.3, 0.4] # list of len 4 (for 4 possible actions <l,r,u,d>)

init: # How to initialize values (opt(1)/pes(-1)/zero/rand)
  Q: zero
  V: zero

# number of times to repeat Q learning algos due to randomness in e-greedy policy
exp:
  repeat: 50

value_iteration_theta: 0.00001

q_learning:
  steps: 4000 # episodes
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99 # epsilon decay rate
  interval: 40 # epsilon decay interval

dq_learning:
  steps: 4000
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 40

pessimistic_q_learning:
  steps: 4000
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 40
  pessimism_coeff: 0.1
