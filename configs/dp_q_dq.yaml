model: filev2 # one_state, five_state, gridworld, file
# if using gridworld, specify the gridworld env name
# eg. MiniGrid-SimpleCrossingS11N5-v0 MiniGrid-Empty-8x8-v0
# MiniGrid-LavaCrossingS11N5-v0 MiniGrid-LavaGapS7-v0 MiniGrid-FourRooms-v0
env: MiniGrid-Empty-8x8-v0
map_path: ./maps/new_stoch_r.txt
gamma: 0.9

env_mods:
  # (num of timesteps after reward is actually returned, except for terminal state)
  delay: None # inf, int, None

init: # How to initialize values (opt(1)/pes(-1)/zero/rand)
  Q: zero
  V: zero

# number of times to repeat Q learning algos due to randomness in e-greedy policy
exp:
  repeat: 1

value_iteration_theta: 0.0001

q_learning:
  steps: 300 # episodes
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99 # epsilon decay rate
  interval: 30 # epsilon decay interval

dq_learning:
  steps: 300
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 30

pessimistic_q_learning:
  steps: 300
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 30
  pessimism_coeff: 0.1

mmq_learning:
  steps: 300
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 30
  estimators: 4
  buffer_size: 100
  replay_size: 10

meanvar_q_learning:
  steps: 300
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 30
  coeff: 0.1