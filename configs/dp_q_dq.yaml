model: filev2 # one_state, five_state, gridworld, file, filev2
# if using gridworld, specify the gridworld env name
# eg. MiniGrid-SimpleCrossingS11N5-v0 MiniGrid-Empty-8x8-v0
# MiniGrid-LavaCrossingS11N5-v0 MiniGrid-LavaGapS7-v0 MiniGrid-FourRooms-v0
env: MiniGrid-Empty-8x8-v0
map_path: ./maps/new_lava_large_three_cols.txt
gamma: 0.9

env_mods:
  # (num of timesteps after reward is actually returned, except for terminal state)
  delay: None # inf, int, None
  state_bonus: False
  action_bonus: False
  distance_bonus: False

init: # How to initialize values (opt(1)/pes(-1)/zero/rand)
  Q: rand
  V: rand

# number of times to repeat Q learning algos due to randomness in e-greedy policy
exp:
  repeat: 5

value_iteration_theta: 0.0001

q_learning:
  steps: 100 # episodes
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99 # epsilon decay rate
  interval: 40 # epsilon decay interval
  buffer_size: 100
  replay_size: 10

dq_learning:
  steps: 100
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 40
  buffer_size: 100
  replay_size: 10

pessimistic_q_learning:
  steps: 100
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 40
  pessimism_coeff: 0.1

mmq_learning:
  steps: 10000
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 40
  estimator_pools: [4]
  buffer_size: 100
  replay_size: 10

mmbq_learning:
  steps: 10000
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 40
  max_estimators: 5
  cum_len: 5
  bandit_lr: 0.05
  buffer_size: 100
  replay_size: 10

meanvar_q_learning:
  steps: 100
  alpha: 0.05
  epsilon: 0.1
  decay: 0.99
  interval: 40
  coeff: 0.1
